---
layout: distill
title: "Lecture 15: Statistical and Algorithmic Foundations of Deep Learning "
description: Classic network learning algorithms.
date: 2019-03-06

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: # author's full name
    url:   # optional URL to the author's homepage
  - name:
    url: 
  - name:
    url: 

editors:
  - name: # editor's full name
    url:  # optional URL to the editor's homepage
---

## Equations

Independently of SBNs, people approached the problem of training deep architectures from a different lens and arrived at Deep Belief Nets. As discussed earlier, approaching the problem using exact inference is problematic due to the explaining away effect. Instead, Deep Belief Nets approach the problem as a hybrid graphical models which learns to extract deep hierarchical representations of the training data by stacking RBMs trained in a greedy manner followed by some ad-hoc fine-tuning.

More concretely, DBNs represent a joint probability distribution 

$$
P(v,h^1,h^2,h^3) = P(h^2,h^3) P(h^1|h^2) P(v|h^1) 
$$

where v are the visible variables and $ h^1 $, $ h^2 $ and $ h^3 $ are the hidden variables as shown in the figure below. Here, $ P(h^2,h^3) $ is an RBM and the conditionals $ P(h^1 \vert h^2) $ and $ P(v \vert h^1) $ are represented as sigmoid activations over a linear layer. As described earlier, directly performing inference and maximizing the likelihood of the joint is problematic. Hence we resort to a greedy layer-wise pretraining strategy described below:

<figure id="DBN" class="l-body-outset">
      <img src="{{ 'assets/img/notes/lecture-15/DBN.png' | relative_url }}" />
</figure>

### Layer-wise pre-training

The principle of greedy layer-wise unsupervised pre-training can be described as follows:
  - **Pre-train and freeze the first RBM** : Train the first layer as a standard RBM with the data being fed in at the visible layers. 
  - **Obtain the hidden layer samples $ p(h^1 \vert h^0) $** : Use the trained RBM to get corresponding hidden samples for each data point by computing $p(h^1 \vert h^0)$ for each point in the training set. 
  - **Train another RBM using samples obtained** : Use the hidden layer samples obtained in the previous step to train another RBM where these samples are fed in at its visible layers. Note that, the first RBM remains fixed during this step.
  - **Iterate the previous two steps** : Repeat the procedure in previous two steps to train the desired number of layers by training successive RBMs using the hidden samples from the previous layer.
Note that at any point, the last RBM (say the $N^{th}$ RBM) trained can be unrolled infinitely to produce an infinite belief network with the first N-1 layers given by the weights of the first N-1 trained RBMs and all the layers after that being represented using tied weights given by the $N^{th}$ RBM. Hence, each additional RBM trained can be viewed as untying an additional layer and freezing that layer of weights while fine-tuning all the following layers of the infinite belief network. Eg. In the figure below, the first 2 layers have been frozen and untied from the training procedure while all the layers starting from W3 are being trained as a standard RBM. 
After pre-training the N RBMs, we simply stack them in the order in which they were trained to obtain an initialization of the DBN. 
<figure id="Pre-training" class="l-body-outset">
      <img src="{{ 'assets/img/notes/lecture-15/pretraining.png' | relative_url }}" />
</figure>
### Fine-tuning
Since the procedure followed above is pretty ad-hoc, it is unlikely to lead to a good probabilistic/generative model. However, the representations learnt can be useful for other downstream task. Some examples of possible use cases for these representations could be :
  - **Unsupervised Learning (DBN -> Autoencoder)**: Autoencoders are a useful model class for compressing the input data to a low dimensional representation from which input can be decoded back without much loss of information. DBNs by themselves might not be able to find an ideal compression but the weights trained using the pre-training procedure might be able to provide a good initialization for the model. The procedure for fine-tuning a DBN to obtain an Autoencoder can be described as follows:
    - Pre-train a stack of RBMs in a greedy layer-wise fashion as described in the previous section with successively fewer hidden units in each successive RBM as shown in the figure below.
    - Unroll the RBMs to create an autoencoder as shown in the figure below.
    - Fine-tune the parameters by backpropagating the reconstruction error and optimizing using gradient descent.
  - **Supervised Learning (DBN -> classifier)**: The representations learnt by the DBN might not be very useful for classifying the data, but it can serve as a good initialization to further fine-tune the network to obtain useful representations. The procedure in this case can be described as follows:
    - Pre-train a stack of RBMs in a greedy layer-wise fashion as in the autoencoder case.
    - Unroll the RBMs to create a feed-forward classifier. 
    - Fine-tune the parameters by backpropagating the classification error and optimizing by gradient descent.

<figure id="ae" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-15/ae-pretrain.png' | relative_url }}" />
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-15/ae-unroll.png' | relative_url }}" />
    </div>
  </div>
</figure>

## DBM: Deep Boltzmann Machines
<figure id="DBM" class="l-body-outset">
      <img src="{{ 'assets/img/notes/lecture-15/DBM.png' | relative_url }}" />
</figure>
DBMs are a fully undirected extension of DBNs. They can be trained using a similar procedure as RBMs using MCMC or using a variational approximation of the data distribution and subsequently doing greedy layerwise pre-training as in the case of DBNs. As with DBNs these can also be used to initialize other networks for downstream tasks. 

## RBM v/s optimization steps
We have so far looked at the optimization in RBMs/infinite belief nets as an inference procedure to progressively infer the values of the hidden states given the observed/visible states. Now imagine that the input/observed variables given to the RBM are actually the initial parameters of another model. In this case each unrolled RBM layer can be seen as an update or optimization step on the parameters of the other model. This meta-learning setup has been proposed recently in <d-cite key="Anrychowicz2016"></d-cite> using RNNs to unroll the optimization steps. <d-cite key="domke2012draw"></d-cite> proposed a method to find the optimal steps to take given a fixed number of optimization steps. This can be represented as:

$$
y*(x,w) = opt-alg_y E(y,x;w)
$$

Here, y* can be seen as a non-linear differentiable function of the inputs and weights. Hence, we can impose some loss and optimize it as any other standard computation graph using backprop. Similarly, message passing based inference algorithms which can be truncated and converted into computational graphs and subsuquently optimizing the objective containing the approximate marginals (computed using the truncated message passing) directly to obtain better message passing and faster convergence.
## Combining sequential NNs and GMs
### Hybrid : RNN+HMM
The setup of this framework is given in the figure below:
<figure id="HMM_RNN" class="l-body-outset">
      <img src="{{ 'assets/img/notes/lecture-15/HMM_RNN.png' | relative_url }}" />
</figure>
The objective, model, inference and learning for the above model can be described as below:
  - **Objective** : Simply maximize the log likelihood of the observed data
  - **Model** : 

